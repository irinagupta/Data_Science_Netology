{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. \n",
    "\n",
    "### Обязательная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем парсить страницу со свежеми новостям на [habr.com/ru/all/](https://habr.com/ru/all/).\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "`KEYWORDS = ['python', 'парсинг']`\n",
    "\n",
    " Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы). \n",
    " \n",
    "В итоге должен формироваться датафрейм со столбцами: <дата> - <заголовок> - <ссылка>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url, query):\n",
    "    \"\"\"Собирает ссылки на статьи в результатах поиска\"\"\"\n",
    "    links_list = []\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'order_by': 'relevance'\n",
    "    }\n",
    "    res = requests.get(URL, params)\n",
    "    time.sleep(0.3)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    search_blocks = soup.find_all('h2', class_='post__title')\n",
    "    for block in search_blocks:\n",
    "        title = soup.find('h2', class_='post__title').text\n",
    "        date = soup.find('span', class_='post__time').text\n",
    "        link = map(lambda x: x.find('a').get('href'), search_blocks)\n",
    "        row = {'date': date, 'title': title, 'link': links}\n",
    "        habr_search = pd.concat([habr_search, pd.DataFrame([row])])\n",
    "    return habr_search\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "URL = 'https://habr.com/ru/search/'\n",
    "habr_search = pd.DataFrame()\n",
    "all_links = []\n",
    "for word in KEYWORDS:\n",
    "    data = get_all_links(URL, word)\n",
    "#     links = get_all_links(URL, word)\n",
    "#     all_links = all_links + links\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habr_search = pd.DataFrame()\n",
    "for links in all_links:\n",
    "    soup = BeautifulSoup(requests.get(links).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    date = soup.find('span', class_='post__time').text\n",
    "    title = soup.find('span', class_='post__title-text').text\n",
    "    row = {'date': date, 'title': title, 'link': links}\n",
    "    habr_search = pd.concat([habr_search, pd.DataFrame([row])])  \n",
    "habr_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(url, query):\n",
    "    \"\"\"Собирает ссылки на статьи в результатах поиска\"\"\"\n",
    "    links_list = []\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'order_by': 'relevance'\n",
    "    }\n",
    "    res = requests.get(URL, params)\n",
    "    time.sleep(0.3)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    search_blocks = soup.find_all('h2', class_='post__title')\n",
    "    links_list = list(map(lambda x: x.find('a').get('href'), search_blocks))\n",
    "    return links_list\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "URL = 'https://habr.com/ru/search/'\n",
    "all_links = []\n",
    "for word in KEYWORDS:\n",
    "    links = get_all_links(URL, word)\n",
    "    all_links = all_links + links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habr_search = pd.DataFrame()\n",
    "for links in all_links:\n",
    "    soup = BeautifulSoup(requests.get(links).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    date = soup.find('span', class_='post__time').text\n",
    "    title = soup.find('span', class_='post__title-text').text\n",
    "    article_text = soup.find('div', class_='post__text post__text-html post__text_v1').text\n",
    "    row = {'date': date, 'title': title, 'link': links, 'article_text': article_text}\n",
    "    habr_search = pd.concat([habr_search, pd.DataFrame([row])])  \n",
    "habr_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительная часть (необязательная)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обязательная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса [Avast Hack Ckeck](https://www.avast.com/hackcheck/).\n",
    "Список email-ов задаем переменной в начале кода:  \n",
    "`EMAIL = [xxx@x.ru, yyy@y.com]`\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <почта> - <дата утечки> - <источник утечки> - <описание утечки>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(url, mail):\n",
    "    \"\"\"проверяет список e-mail адресов на утечку\"\"\"\n",
    "#     links_list = []\n",
    "    params = {\n",
    "        'email': mail\n",
    "    }\n",
    "    res = requests.post(URL, params)\n",
    "    time.sleep(0.3)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "#     search_blocks = soup.find_all('h2', class_='post__title')\n",
    "#     links_list = list(map(lambda x: x.find('a').get('href'), search_blocks))\n",
    "    return answer\n",
    "\n",
    "URL = 'https://www.avast.com/hackcheck/'\n",
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']\n",
    "for mail in EMAIL:\n",
    "    answer = get_answer(URL, mail)\n",
    "    answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://digibody.avast.com/v1/web/leaks'\n",
    "params = {'email': 'xxx@x.ru'}\n",
    "resp = requests.post(URL, params)\n",
    "resp\n",
    "# soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительная часть (необязательная)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.  \n",
    "Документация к API VK: https://vk.com/dev/methods\n",
    ", вам поможет метод [wall.get](https://vk.com/dev/wall.get)```\n",
    "GROUP = 'netology'\n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "```\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
