{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling\n",
    "\n",
    "([Открыть в Colab](https://colab.research.google.com/drive/1c3QoUsRRCes9RKttQ3-SuSlFQZ3jAngz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Обучим две различные символьные модели для генерации динозавров:\n",
    "* модель на символьных биграмах\n",
    "* ***RNN***-модель.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhuchengtyrannus\n",
      "Ziapelta\n",
      "Zigongosaurus\n",
      "Zizhongosaurus\n",
      "Zuniceratops\n",
      "Zunityrannus\n",
      "Zuolong\n",
      "Zuoyunlong\n",
      "Zupaysaurus\n",
      "Zuul"
     ]
    }
   ],
   "source": [
    "!cat dinos.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<aachenosaurus>', '<aardonyx>', '<abdallahsaurus>', '<abelisaurus>', '<abrictosaurus>', '<abrosaurus>', '<abydosaurus>', '<acanthopholis>', '<achelousaurus>', '<acheroraptor>']\n"
     ]
    }
   ],
   "source": [
    "names = ['<' + name.strip().lower() + '>' for name in open('dinos.txt').readlines()]\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим частоту каждого символа в корпусе имен динозавров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [char for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2487, 's': 2285, 'u': 2123, 'o': 1710, 'r': 1704, '<': 1536, '>': 1536, 'n': 1081, 'i': 944, 'e': 913, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'a', 'c', 'h', 'e', 'n', 'o', 's', 'u', 'r', '>', 'd', 'y', 'x', 'b', 'l', 'i', 't', 'p', 'v', 'm', 'g', 'f', 'j', 'k', 'w', 'z', 'q']\n"
     ]
    }
   ],
   "source": [
    "print(list(freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2487),\n",
       " ('s', 2285),\n",
       " ('u', 2123),\n",
       " ('o', 1710),\n",
       " ('r', 1704),\n",
       " ('<', 1536),\n",
       " ('>', 1536),\n",
       " ('n', 1081),\n",
       " ('i', 944),\n",
       " ('e', 913)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to estimate probabilty of character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a) = 0.1160\n"
     ]
    }
   ],
   "source": [
    "print('p(a) = %1.4f' %unigram_prob('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим условную вероятность каждого символа в зависимости от того, какой символ стоял на предыдущей позиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'u': 791, 'n': 347, 't': 204, 's': 171, 'l': 138, '>': 138, 'r': 124, 'c': 100, 'p': 89, 'm': 68, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfreq['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим условные вероятности с помощью MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a a) = 0.0044\n",
      "p(a b) = 0.0097\n",
      "p(a u) = 0.3181\n"
     ]
    }
   ],
   "source": [
    "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob['a'].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "\n",
    "1) Напишите функцию, которая генерирует имя динозавра **фиксированной** длины. Используйте '<' как начальный символ.\n",
    "\n",
    "2) Напишите функцию, которая генерирует имя динозавра любой дины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<seeetoyl>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fixed_name_generator(name_len):\n",
    "    dinname = '<'\n",
    "    for i in range(name_len):\n",
    "        new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        while new_char in ['<', '>']:\n",
    "            new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        dinname += new_char\n",
    "\n",
    "    dinname += '>'\n",
    "    return dinname\n",
    "\n",
    "fixed_name_generator(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<vopaursqurngurotalitodrs>'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def any_name_generator():\n",
    "    dinname = '<'\n",
    "    while dinname[-1] != '>':\n",
    "        new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        while new_char in ['<']:\n",
    "            new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        dinname += new_char\n",
    "\n",
    "    return dinname\n",
    "\n",
    "any_name_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<tozaurngosarurapszionochelovesusausausauras>'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_name_generator(max_len):\n",
    "    dinname = '<'\n",
    "    name_len = np.random.choice(max_len) + 1\n",
    "    for i in range(name_len):\n",
    "        new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        while new_char in ['<', '>']:\n",
    "            new_char = cprob[dinname[-1]].generate()\n",
    "\n",
    "        dinname += new_char\n",
    "\n",
    "    dinname += '>'\n",
    "    return dinname\n",
    "\n",
    "random_name_generator(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реккурентные нейронные сети (RNN)\n",
    "\n",
    "Исходная последовательность:\n",
    "\n",
    "$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n",
    "\n",
    "Для каждого входного значения $x_{1:i}$ получаем на выходе $y_i$:\n",
    "\n",
    "$y_i = RNN(x_{1:i})$, $y_i \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "Для всей последовательности $x_{1:n}$:\n",
    "\n",
    "$y_{1:n} = RNN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R$ - рекурсивная функция активации, зависящая от двух параметров: $x_i$ и $s_{i-1}$ (вектор предыдущего состояния)\n",
    "\n",
    "$RNN^{*}(x_{1:n}, s_0) = y_{1:n}$\n",
    "\n",
    "$y_i = O(s_i) = g(W^{out}[s_{i} ,x_i] +b)$\n",
    "\n",
    "$s_i = R(s_{i-1}, x_i)$\n",
    "\n",
    "$s_i = R(s_{i-1}, x_i) = g(W^{hid}[s_{i-1} ,x_i] +b)$  -- конкатенация $[s_{i-1}, x]$\n",
    "\n",
    "$x_i \\in \\mathbb{R}^{d_{in}}$, $y_i \\in \\mathbb{R}^{ d_{out}}$, $s_i \\in \\mathbb{R}^{d_{hid}}$\n",
    "\n",
    "$W^{hid} \\in \\mathbb{R}^{(d_{in}+d_{out}) \\times d_{hid}}$, $W^{out} \\in \\mathbb{R}^{d_{hid} \\times d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим языковую модель на основе RNN с помощью pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DinosDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        with open('dinos.txt') as f:\n",
    "            content = f.read().lower()\n",
    "            self.vocab = sorted(set(content)) + ['<', '>']\n",
    "            self.vocab_size = len(self.vocab)\n",
    "            self.lines = content.splitlines()\n",
    "        self.ch_to_idx = {c:i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_ch = {i:c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        #teacher forcing\n",
    "        x_str = '<' + line \n",
    "        y_str = line + '>' \n",
    "        x = torch.zeros([len(x_str), self.vocab_size], dtype=torch.float)\n",
    "        y = torch.empty(len(x_str), dtype=torch.long)\n",
    "        for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)):\n",
    "            x[i][self.ch_to_idx[x_ch]] = 1\n",
    "            y[i] = self.ch_to_idx[y_ch]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = DinosDataset()\n",
    "trn_dl = DataLoader(trn_ds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardonyx'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       " tensor([ 1,  1, 18,  4, 15, 14, 25, 24, 28]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '<', 28: '>'}\n"
     ]
    }
   ],
   "source": [
    "print(trn_ds.idx_to_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trn_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 29])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем модель, функцию потерь и алгоритм оптимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, h_prev, x):\n",
    "        combined = torch.cat([h_prev, x], dim = 1) # concatenate x and h\n",
    "        h = torch.tanh(self.dropout(self.i2h(combined)))\n",
    "        y = self.i2o(h)\n",
    "        return h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(trn_ds.vocab_size, hidden_size, trn_ds.vocab_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn](images/dinos3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sample(model):\n",
    "    model.eval()\n",
    "    word_size=0\n",
    "    end_char_idx = trn_ds.ch_to_idx['>'] # чтобы понимать, когда слово кончилось\n",
    "    start_char_idx = trn_ds.ch_to_idx['<'] # seed для начала слова\n",
    "    with torch.no_grad():\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x = h_prev.new_zeros([1, trn_ds.vocab_size])\n",
    "        \n",
    "        # \"сгенерирвали\" начало слова\n",
    "        indices = [start_char_idx]\n",
    "        x[0, start_char_idx] = 1\n",
    "        predicted_char_idx = start_char_idx\n",
    "        \n",
    "        while predicted_char_idx != end_char_idx and word_size != 50:\n",
    "            h_prev, y_pred = model(h_prev, x)\n",
    "            y_softmax_scores = torch.softmax(y_pred, dim=1)\n",
    "            \n",
    "            np.random.seed(np.random.randint(1, 5000))\n",
    "            idx = np.random.choice(np.arange(trn_ds.vocab_size), p=y_softmax_scores.cpu().numpy().ravel())\n",
    "            indices.append(idx)\n",
    "            \n",
    "            x = (y_pred == y_pred.max(1)[0]).float() # нетривиальное решение: подаём не всегда то же, что выдали\n",
    "            \n",
    "            predicted_char_idx = idx\n",
    "            \n",
    "            word_size += 1\n",
    "        \n",
    "        if word_size == 50:\n",
    "            indices.append(newline_idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def print_sample(sample_idxs):\n",
    "    [print(trn_ds.idx_to_ch[x], end ='') for x in sample_idxs]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим получившуюся модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for line_num, (x, y) in enumerate(trn_dl):\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        h_prev = torch.zeros([1, hidden_size], dtype=torch.float, device=device)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        for i in range(x.shape[1]):\n",
    "            h_prev, y_pred = model(h_prev, x[:, i])\n",
    "            loss += loss_fn(y_pred, y[:, i])\n",
    "            \n",
    "        if (line_num+1) % 100 == 0:\n",
    "            print_sample(sample(model))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataset='dinos', epochs=1):\n",
    "    for e in range(1, epochs+1):\n",
    "        print('Epoch:{}'.format(e))\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "<\n",
      "ag>\n",
      "<hgiurohsapr>\n",
      "<bnsrs>\n",
      "<inrud>\n",
      "<qel>\n",
      "<<eotsaur>\n",
      "<paros>\n",
      "<taurug>\n",
      "<amvntbshurus>\n",
      "<lyurus>\n",
      "<anras>\n",
      "<auauadstaraurun>\n",
      "<artotaurus>\n",
      "<rupysaurud>\n",
      "<lalrasaunus>\n",
      "\n",
      "Epoch:2\n",
      "<ahislhc>\n",
      "<aaurudtseurus>\n",
      "<zurortaraurus>\n",
      "<htcltsaurus>\n",
      "<ancrussurus>\n",
      "<dtrkvpurus>\n",
      "<hrcisauruc>\n",
      "<shbresaurus>\n",
      "<xstasurngssusus>\n",
      "<suaopaurus>\n",
      "<tcdssaurus>\n",
      "<arcgyrueseurus>\n",
      "<lyriosiprus>\n",
      "<sapesaurun>\n",
      "<ctamasar>\n",
      "\n",
      "Epoch:3\n",
      "<otlrus>\n",
      "<drpnysaurun>\n",
      "<manrasaurus>\n",
      "<argulaesas>\n",
      "<puattoratopy>\n",
      "<lottanamrus>\n",
      "<frcatocaurus>\n",
      "<tcgvntdaurus>\n",
      "<pnyooaurus>\n",
      "<acucrnonrus>\n",
      "<lltlgasaurus>\n",
      "<stugisaurus>\n",
      "<suanatrus>\n",
      "<euadrsaurus>\n",
      "<asaotaurus>\n",
      "\n",
      "Epoch:4\n",
      "<llosaurus>\n",
      "<tcglaaurus>\n",
      "<aaurasaurus>\n",
      "<anxqtgiurus>\n",
      "<ppuusaurus>\n",
      "<aloasgsaurul>\n",
      "<dqleasaurus>\n",
      "<supneositaurus>\n",
      "<hnaesasaurus>\n",
      "<ararpicter>\n",
      "<ouctsosaurus>\n",
      "<mtrpasaqrus>\n",
      "<huccspaurus>\n",
      "<aterus>\n",
      "<brstonaurus>\n",
      "\n",
      "Epoch:5\n",
      "<prramidaurus>\n",
      "<aauraraurus>\n",
      "<apwsrjiurus>\n",
      "<rnwuntotah>\n",
      "<amsasaurus>\n",
      "<aiitenasaurus>\n",
      "<strngssiysaurus>\n",
      "<lcatasesapopc>\n",
      "<cugicaorus>\n",
      "<aussomaurus>\n",
      "<qtuboraurus>\n",
      "<tccsraurus>\n",
      "<atantt>\n",
      "<attoolaurus>\n",
      "<qpuaopaesaurus>\n",
      "\n",
      "Epoch:6\n",
      "<smapesaurus>\n",
      "<xpuitosaurus>\n",
      "<tnrusaopaurus>\n",
      "<ucaticaurus>\n",
      "<sanvsourus>\n",
      "<epurwsaurus>\n",
      "<lamscourus>\n",
      "<maresgoatsrus>\n",
      "<bpuronaurus>\n",
      "<pstananrus>\n",
      "<euaestasaurus>\n",
      "<aaurulaurus>\n",
      "<rnytosaurus>\n",
      "<ftdrlsaurus>\n",
      "<dronaurus>\n",
      "\n",
      "Epoch:7\n",
      "<tartsopaurus>\n",
      "<pttangansdsaurus>\n",
      "<aigtirasaurus>\n",
      "<sturosootaurus>\n",
      "<iecgodaurts>\n",
      "<malkosaurus>\n",
      "<ouassrngsaurus>\n",
      "<suahncpaurus>\n",
      "<atiallauros>\n",
      "<kvltasiurus>\n",
      "<lurdnosciocaurus>\n",
      "<aerhangtraurus>\n",
      "<puaosburus>\n",
      "<ztenstcaurus>\n",
      "<sltaaopaor>\n",
      "\n",
      "Epoch:8\n",
      "<gnapcaurus>\n",
      "<stngosaurus>\n",
      "<tcgncgsdaurus>\n",
      "<mandonhcturus>\n",
      "<bttopaurus>\n",
      "<krsocaurus>\n",
      "<sltaassaurus>\n",
      "<ategviurus>\n",
      "<mapolytiteo>\n",
      "<ihcdtasaurus>\n",
      "<ancunnatmouaurus>\n",
      "<lgrum>\n",
      "<shmtuapasrus>\n",
      "<gucnusams>\n",
      "<hmalacrouext>\n",
      "\n",
      "Epoch:9\n",
      "<maurotaurus>\n",
      "<jhcescoosaurus>\n",
      "<atcmasanturus>\n",
      "<snbstnysaurus>\n",
      "<kchogopgurustus>\n",
      "<hegtaoturus>\n",
      "<snctosuurus>\n",
      "<aihcisaurus>\n",
      "<sicigrpaurus>\n",
      "<pultssaurus>\n",
      "<sprosauros>\n",
      "<atgocauros>\n",
      "<selataurus>\n",
      "<strngsaurus>\n",
      "<stanraurus>\n",
      "\n",
      "Epoch:10\n",
      "<valrocaurus>\n",
      "<tcgurtasaurus>\n",
      "<lyurosdsrus>\n",
      "<saugocasatrus>\n",
      "<haugosaurus>\n",
      "<llsrpturus>\n",
      "<angamkurus>\n",
      "<conaonaurus>\n",
      "<huntdseurus>\n",
      "<hxtirotbimeg>\n",
      "<aohtadoobaurus>\n",
      "<sartotente>\n",
      "<doravturus>\n",
      "<hmahocaurus>\n",
      "<lbpeselaors>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fuaastariunhasaurus>\n"
     ]
    }
   ],
   "source": [
    "print_sample(sample(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "Rewrite the sampling function so that pangrams (words that contain each character of the alphabet only once)\n",
    "\n",
    "### Task 3.\n",
    "Rewrite the sampling function so that is it is possible to change the sampling temperature\n",
    "\n",
    "### Task 4.\n",
    "Implement the beam search for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. Sampling in  RNN: https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
    "2. Coursera course (main source): https://github.com/furkanu/deeplearning.ai-pytorch/tree/master/5-%20Sequence%20Models\n",
    "3. Coursera course (main source): https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Dinosaurus%20Island%20--%20Character%20level%20language%20model%20final%20-%20v3.ipynb\n",
    "4. LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "5. Туториал по модулю LSTM в pytorch: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
